## Kafka 性能优化-高级



<img src="https://tva1.sinaimg.cn/large/008i3skNly1gututqcz69j60xu0lk75o02.jpg" alt="00C8D2A8-4503-4ADE-9201-F21D54EB4961" style="zoom:50%;" />

性能问题主要是三个方面：**网络、磁盘、复杂度**；对于 Kafka 分布式队列，网络、磁盘 是优化的重点
**解决方案：**并发、压缩、批量、缓存

#### Kafka 为什么快？

1. 顺序读写磁盘
2. 零拷贝网络和磁盘
3. 数据批量压缩，传输，减少网络 IO 损耗
4. Partition 并行和可扩展
5. 高效的文件数据结构设计
6. 优秀的网络模型（基于 Java NIO）



#### 1.顺序读写磁盘

完成一次磁盘IO，经过 寻道、旋转、数据传输三个步骤，如果写磁盘时 省去寻道、旋转可以极大地提高磁盘读写的性能。虽然使用硬盘存储，但仍然速度很快。
Kafka 采用 顺序写文件的方式来提高磁盘写入性能，基本减少了 磁盘寻道和旋转的次数。
Kafka 中 每个分区是一个有序的，不可变的消息序列，新消息不断追加到 Partition 的末尾，在 Kafka 中 Partition 只是一个逻辑概念，将 Partition 划分为多个 Segment，每个 Segment 对应一个物理文件，Kafka 对 segment 文件追加写，这就是顺序写文件。 



#### 2.零拷贝网络和磁盘

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuu3hq52j61020gu40s02.jpg" alt="CE3A9512-3107-4375-9982-6E6E677FE7E4" style="zoom:50%;" />

传统的 IO流程，先读取 网络IO，再写入磁盘IO，实际需要将数据 Copy 四次


1. 第一次：读取 磁盘文件 到 操作系统 内核缓冲区；DMA搬运的
2. 第二次：将 内核缓冲区 的数据，copy 到 应用程序的 buffer；CPU 
3. 第三步：将 应用程序 buffer 中的数据，copy 到 socket buffer (网络发送缓冲区); CPU 
4. 第四次：将 socket buffer 的数据，copy 到 网卡，由 网卡进行网络传输。  DMA 
磁盘 —> 内核 buf—> 用户 buf—> Socket buf—> 网卡
内核 buf —> Socket buf —> 网卡

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuue7s5tj60y00lyq3y02.jpg" alt="BE3BEF3C-ABB9-4285-B3E7-6C9FB4647779" style="zoom: 33%;" /><img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuvkxwwjj60fe0kit9402.jpg" alt="46ACAB27-0054-4A38-98D2-A82C58E3FEE1" style="zoom:53%;" />

Kafka 实现零拷贝，在模型中上下文切换数量减少一倍，只有 2次copy，只有DMA来进行数据搬运，而不需要CPU。
**第一次通过 DMA**，从 磁盘 —> 内核读缓冲区  
**第二次根据 Socket的描述符信息**，使用 DMA 直接从 内核缓冲区—>写入到 网卡缓冲区 
零拷贝是尽量去减少上面数据的拷贝次数，减少拷贝的 CPU开销，减少用户态内核态的上下文切换次数，从而优化数据传输的性能。

**同一份数据的 传输次数从 四次变成两次，并且没有通过 CPU来进行数据搬运，所有数据都通过 DMA来进行传输**。
没有在内存层面去复制数据，所以这个方法也被称为零拷贝。 



#### DMA（Direct Memory Access） 技术：

​		是在主板上放⼀块独立的芯片，在进行 内存 和 I/O设备的数据传输 时，不再通过 CPU 控制数据传输，而直接通过 DMA控制器  ，传统的 从硬盘读取数据，然后再通过 网卡向外发送，需要进行四次数据传输，其中有两次是发生在内存里的缓冲区和对应的硬件设备之间，没法节省掉。但是还有两次，完全是通过 CPU在内存里面进行数据复制，
在 Kafka里，通过 Java 的 NIO 里面 FileChannel的 transferTo方法调用，可以不用把 数据复制到应用程序的内存里面。通过DMA的方式，可以把 数据从内存缓冲区 直接写到 网卡的缓冲区里面。

 		DMAC 是一个 协处理器芯片，通过这个芯片，CPU 只需要告诉 DMAC，我们要 传输什么数据，从哪里来，到哪里去，就可以放心离开了；后续的实际数据传输工作，都会有 DMAC 来完成。随着现代计算机各种外设硬件越来越多， 光一个通用的 DMAC 芯片不够了，我们在各个外设上都加上了 DMAC 芯片，使得 CPU 很少再需要关心数据传输的工作了。  数据传输工作用不到多少 CPU 核新的“计算”功能，发送数据使用sendfile： 

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuwfbh9xj612s0je76g02.jpg" alt="1C7E2EEF-E6D9-40ED-B269-880447776636" style="zoom:50%;" />



#### Page Cache 作用 

* 缓存最近被访问的数据 
* 预读功能 

如果 producer 生产与 consumer消费 速度差不多，可以只对 broker page cache 读写来完成整个生产--消费过程，磁盘访问非常少，producer 生产消息到 Broker ，Broker 按偏移量写入数据，此时数据会先写入 page cache内存区域。consumer 消费消息时，Broker 将数据从  page cache 传输到 Socket buffer，再将 Socket Buffer的数据 copy到网卡，由网卡进行网络传输。 page cache中的数据会随着内核中 flusher 线程的调度写回到磁盘，不用担心数据丢失。如果 consumer要消费的消息不在page cache里，会去磁盘读取。



#### 3.批量发送与压缩 

Producer 向 Broker 发送消息不是一条一条的发送， 而是进行批量发送，将消息缓存在本地，等到定条件 发送到 Broker；
**1.消息条数；  2.固定一段时间**

Producer 执行流程如下图：

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuxm5jfdj62420q843l02.jpg" alt="B84EC345-6856-45D9-9253-F904252F6482" style="zoom:80%;" />

* Serialize：序列化传递的消息 (序列化后可提高网络传输效率)
* Partition：决定将消息写入主题的哪个分区
* Compress：压缩消息，提高传输速度（生产者—>代理），提高吞吐量，降低延迟并提高磁盘利用率
* Accumulate：消息累计器，每个 Partition维护一个双端队列，队列保存将要发送的批次数据，Accumulate将数据累计到一定数量，或在一定时间内，将数据以批次的方式发送出去， 主题中的每个分区都有一个单独的累加器 / 缓冲区。
* Group Send：记录每个分区的消息数量，当达到定义大小 或达到定义的延迟时间时，将它们发送到的分组

**压缩作用：** 减少传输的数据量，减轻对网络的传输压力 
Producer、Broker 和 Consumer  使用相同的压缩算法， producer 向 Broker 写入数据，Consumer 向 Broker 读取数据时不用解压缩，当消息发送到Consumer 后才解压，这样将  节省大量网络开销
Producer 压缩之后，Consumer 需进行解压，虽然增加 CPU的工作，但在对大数据处理上，瓶颈在网络而不是CPU，所以这个成本是值得的
注意：「批量发送」+「数据压缩」一起使用，单条做数据压缩的话，效果不明显 



#### 4.Partition 并行和可扩展

​		每个 Partition是一个队列，同一个 Group下不同 Consumer并发消费 Paritition，Paritition分区是并行度最小单元，每增加一个 Paritition就增加了一个消费的并发。 Kafka 具有分区分配算法—StickyAssignor，保证分配尽量均衡，每一次重分配结果尽量与上一次分配结果保持一致。各个 Broker和 Consumer的处理不至于出现太大倾斜

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuy04fzfj61bw0fyq6302.jpg" alt="6B3C5626-E274-4DC7-942B-9C48BF4CC9FF" style="zoom:80%;" />

**分区多导致的问题：**
客户端/服务器端 需要使用更多的内存：客户端 producer 有个参数 batch.size，默认是 16KB。它会为每个分区缓存消息，一旦满了就打包将消息批量发出。因为这个参数是分区级别的，如果分区数量变多，则 缓存所需的内存也会变得更多。

**恢复数据慢：**分区越多，每个 Broker上分配的分区也就越多，当发生 Broker 宕机，那么恢复时间将很长



#### 5.高效的文件数据结构

消息以 Topic为单位进行归类，各个 Topic之间是彼此独立的，互不影响。每个 Topic可以分为一个或多个分区，每个分区各自存在一个记录消息数据的日志文件,每个分区日志在物理上按大小被分成多个 Segment。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuybbil5j61300ha0us02.jpg" alt="CFBF5196-26D1-477E-A43F-D47B2E727C71" style="zoom:80%;" />

segment file 组成： index file 和 data file， 2 个文件一一对应，成对出现（索引文件 .index ，数据文件 .log）
segment 文件命名规则：partition 全局的第一个 segment 从 0 开始，后续每个 segment 文件名为上一个 segment 文件最后一条消息的 offset 值。数值最大为 64 位 long 大小，19 位数字字符长度，没有数字用 0 填充。index 采用稀疏索引，这样每个 index文件大小有限。Kafka 采用 mmap的方式，直接将 index文件映射到内存，这样对 index 就不需要操作磁盘 IO。mmap的 Java 实现对应MappedByteBuffer 。

#### Mmap: 一种 内存映射文件的方法

​		将一个文件或其它对象映射到 进程的地址空间，实现 文件磁盘地址和 进程虚拟地址空间中一段虚拟地址 一一对映关系。实现这样的映射关系后，进程可以采用 指针的方式读写操作这段内存，而系统会自动回写脏页面到对应的文件磁盘上，即对文件进行操作，不必调用 read, write 等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而实现不同进程间的文件共享。

**Memory Mapped Files（mmap）：** 将磁盘文件映射到内存, 用户通过修改内存达到修改磁盘文件的效果
接收来自socket buffer的网络数据，应用进程不需要中间处理、直接进行持久化时。——可以使用mmap内存文件映射。
**原理：**直接利用操作系统的 Page来实现文件到物理内存的直接映射, 完成映射之后对物理内存的操作会被同步到硬盘上（操作系统在适当的时候）。通过 mmap，进程像 读写硬盘一样读写内存（虚拟机内存），也不必关心内存的大小（有虚拟内存兜底），这种方式可以获取很大的 I/O提升，省去了用户空间到 内核空间复制的开销。 

**Mmap问题：**不可靠，写到 mmap中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用 flush的时候才把数据真正的写到硬盘。

**解决：**Kafka 提供一个参数——producer.type 来控制是不是主动flush；如果Kafka写入到mmap之后就立即flush然后再返回Producer叫同步(sync)；写入mmap之后立即返回 Producer不调用flush叫异步(async)。 

#### mmap+write 方式 持久化数

使用 mmap+write 方式代替原来的 read+write 方式，mmap 是一种内存映射文件的方法
Mmap将 磁盘文件映射到 内存，支持读和写，对 内存的操作会反映在磁盘文件上。
原理：将 读缓冲区地址和 用户缓冲区地址进行映射，内核缓冲区 和 应用缓冲区共享，减少从读缓冲区到用户缓冲区的一次CPU拷贝

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuym6baej61280iktau02.jpg" alt="E4CED1AF-9543-48D1-B4CA-3CC422DD259E" style="zoom:50%;" />



#### 虚拟内存 

现代操作系统都使用虚拟内存，使用虚拟地址取代物理地址，这样做的好处是：
1.一个以上的 虚拟地址可以指向同一个物理内存地址
2.虚拟内存空间可大于实际可用的物理地址

利用第一条特性可以把 内核空间地址和 用户空间的虚拟地址映射到同一个物理地址，这样 DMA 就可以 填充对内核和用户空间进程同时可见的缓冲区了；省去内核与用户空间的往来拷贝， Java 也利用操作系统的此特性来提升性能，下面重点看看 Java 对零拷贝都有哪些支持。



#### 6.优秀的网络模型（基于 Java NIO）

Kafka 底层基于 Java NIO，采用 Reactor 线程模型，做的网络模型 RPC
在传统阻塞 IO 模型中问题：
1.每个连接都需要 独立线程处理，当并发数大时，创建线程数多，占用资源；
2.采用阻塞IO模型，连接建立后，若当前线程没有数据可读，线程会阻塞在读操作上，造成资源浪费

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuz1l1vnj60xw0bcgnq02.jpg" alt="E8BFFFF6-0AAE-40F9-937D-CAB93D9F5646" style="zoom:50%;" />

针对传统阻塞IO模型的两个问题，解决方案：

1.基于 池化思想：避免为每个连接创建线程，连接完成后将业务处理交给线程池处理 

2.基于 IO复用模型：多个连接共用同一个阻塞对象，不用等待所有的连接，遍历到有新数据可以处理时，操作系统会通知程序，线程跳出阻塞状态，进行业务逻辑处理 

#### Reactor 线程模型思想： 基于 IO复用 + 线程池

​		Reactor 模型基于 池化思想，当 连接完成后 将业务处理交给线程池处理，避免为 每个连接创建线程；基于 IO 复用模型，多个连接共用同一个阻塞对象，不用等待所有的连接。遍历到有新数据可以处理时，操作系统会通知程序，线程跳出阻塞状态，进行业务逻辑处理。
**实现原理：**Reactor 模式 ：处理并发 I/O  常见模式，将所有要 处理的 IO 事件注册到一个中心 I/O 多路复用器上，同时 主线程/进程 阻塞在多路复用器上；一旦有 I/O 事件到来或准备就绪 (文件描述符或 socket 可读、写)，多路复用器返回并将事先注册的相应 I/O 事件分发到对应的处理器中。
​		Reactor 利用事件驱动机制实现，应用程序需提供相应接口，并注册到Reactor 上，如果相应事件发生，Reactor将主动调用应用程序注册的接口。

epoll 模式已经可以使服务器并发几十万连接的同时，维持极高 TPS，为什么还需要 Reactor 模式？
**原因：**原生的 I/O 复用编程复杂性比较高（ epoll ）  
例如：编程中，处理请求 A 时，可能经过多个 I/O 操作 A1-An，每经过一次 I/O 操作，再调用 I/O 复用时，I/O 复用的调用返回里，可能不再有 A，而返回了请求 B。请求 A 会经常被请求 B 打断，处理请求 B 时，又被 C 打断。使得编程较为复杂。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuzhwym1j60yu0jidih02.jpg" alt="A2B0143B-0D56-4D34-8431-18E33083441D" style="zoom:50%;" />

**Reactor 模型 主要分为三个角色：**
*Reactor： 把 IO 事件分配给对应的 handler 处理
*Acceptor：处理客户端连接事件
*Handler： 处理非阻塞的任务

Acceptor线程用于处理新的连接，Handler 线程处理业务逻辑
I/O 多路复用可以把多个 I/O 阻塞，复用到同一个 select 阻塞上，从而使得系统在单线程情况下，可以同时处理多个客户端请求，不需要创建新的线程，降低了系统的资源开销



#### Reactor线程模型分类：

根据 Reactor的数量和处理资源的线程数量的不同，分为三类：  单Reactor单线程模型；单Reactor多线程模型；多Reactor多线程模型。

单Reactor单线程模型：在Reactor中处理事件，并分发事件，如果连接事件交给acceptor处理，如果是读写事件和业务处理，就交给 handler处理，但始终只有一个线程执行所有的事情。
**问题：**
1.仅用一个线程处理请求，对于多核资源机器来说是有点浪费的。
2.当处理读写任务的线程负载过高后，处理速度下降，事件会堆积，严重的会超时，可能导致客户端重新发送请求，性能越来越差。
3.单线程也会有可靠性的问题。

单Reactor 多线程模型：和单线程模型主要区别： 把业务处理从之前的单一线程脱离出来，换成线程池处理。
Reactor线程 只处理 连接事件和读写事件，业务处理交给 线程池处理，充分利用多核机器的资源，提高性能并且增加可靠性。
**问题：**
1.Reactor线程承担所有的事件，例如监听和响应，高并发场景下单线程存在性能问题

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutuzz3zk9j60w40lsad802.jpg" alt="98B39D17-7A7C-417D-8D4D-49411EA81C6D" style="zoom:50%;" />

多Reactor多线程模型：和单Reactor多线程模型相比，把 Reactor线程拆分 mainReactor和 subReactor两个部分，
mainReactor 只处理连接事件，mainRactor只处理连接事件，用一个线程来处理就好。
读写事件交给 subReactor来处理，处理读写事件的 subReactor个数一般和CPU数量相等，一个subReactor对应一个线程，业务逻辑由线程池处理。
业务逻辑还是由线程池来处理。
这种模型使各个模块职责单一，降低耦合度，性能和稳定性都有提高

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gutv0d4suxj60x60m2q6c02.jpg" alt="710026EB-C05B-4CDF-A05C-47DCDD1422C7" style="zoom:50%;" />

Reactor三种模式形象比喻：
餐厅一般有接待员和服务员，接待员负责在门口接待顾客，服务员负责全程服务顾客
单Reactor单线程模型：接待员和服务员是同一个人，一直为顾客服务。客流量较少适合

1. 单Reactor多线程模型：一个接待员，多个服务员。客流量大，一个人忙不过来，由专门的接待员在门口接待顾客，然后安排好桌子后，由一个服务员一直服务，一般每个服务员负责一片中的几张桌子
2. 多Reactor多线程模型：多个接待员，多个服务员。这种就是客流量太大了，一个接待员忙不过来了



#### 推拉模式：

Producer 与 Broker 是推方式;  推拉模式 指 Comsumer 和 Broker 之间的交互

##### 拉模式的优点：

1.消费者可以根据自身的情况来发起 拉取消息的请求, 假设当前消费者觉得自己消费不过来，可以根据一定的策略停止拉取，或者间隔拉取
2.Broker 只管存生产者发来的消息，消费者主动发起，来一个请求就给它消息，从哪开始拿消息，拿多 少消费者都告诉它
3.适合进行 消息批量发送，拉模式可以参考消费者请求的信息来 决定缓存多少消息之后批量发送;  

##### 拉模式的缺点：

1.消息延迟，消费者去拉取消息，但消费者不知道消息到了没，它只能不断地拉取，但又不能很频繁地请求，太频繁就变成消费者在攻击 Broker , 因此需要降低请求频率，比如隔个 2 秒请求一次.
2.消息忙请求，比如消息隔了几个小时才有，那么在几个小时之内消费者的 请求都是无效的，在做无用功

Kafka 是拉模式，业界基于推模式的消息队列 ActiveMQ 

##### Kafka 应对 拉模式缺点：

Kafka 中的长轮询：
消费者 和 Broker 相互配合，拉取消息请求不满足条件的时候 hold 住，避免多次频繁的拉取动作，当消息一到就提醒返回；拉请求可以设置参数，使消费者在 “长轮询” 中阻塞等待；消费者去 Broker 拉消息，定义一个超时时间，如果有马上返回消息，如果没有消费者等着直到超时，然后再次发起拉消息请求。当没有消息时，Broker 建立一个延迟操作，等条件满足再返回。

这个延迟操作需要 检查机制，查看消息是否已经到了，有消息到了之后该执行的方法，执行完毕后进行什么操作的方法，超时后进行什么操作的方法；判断是否过期就是由 时间轮来推动判断的，在消息写入的时候提醒这些延迟请求消息来了。



**推模式：**消息从 Broker 推向 Consumer，即 Consumer 被动的接收消息，由 Broker 来主导消息的发送

##### 推模式的优点：

1.消息实 时性高， Broker 接受完消息之后可以立马推送给 Consumer
2.对于 消费者使用来说更简单，有消息来了就会推过来

##### 推模式的缺点：

1.推送速率难以适应消费速率，以最快的速度推送消息，当生产者往 Broker 发送消息的速率大于消费者消费消息的速率时，消费者可能消费不过来
2.不同消费者的消费速率不一样，身为 Broker 很难平衡每个消费者推送速率
3.难以根据消费者的状态控制推送速率，适用于消息量不大，消费能力强，要求实时性高的情况