## 多线程-高级

多线程编程详解： https://blog.csdn.net/mu_wind/article/details/113806680



**CAS（Compare And Swap 比较并且替换）**是乐观锁的一种实现方式，线程在读取数据时不进行加锁，
先保存原值，在实际修改数据的瞬间，比较此时的值，是否和原值相等。若相等则回写；若不相等，则重新执行读取流程。

#### CAS产生的问题：

**一、CAS可能有 ABA问题：** （破坏了事务的原子性）

1. 线程1 读取数据A
2. 线程2 读取数据A，把数据A改成数据B
4. 线程3 读取数据B，把数据 B 又改回数据A
6. 线程1 通过CAS比较，发现数据还是 A没变，就写成了自己要改的值

解决：加标志位（类似版本号），设置 自增字段，操作一次自增+1；或搞个时间戳，比较时间戳的值；
每次修改： 数据+版本号；版本号递增，校验时同时校验： 数据 + 版本号 
例如：操作数据库，根据CAS原则本来只要查询原本的值，现在需要一同查出他的标志位版本字段 vision

**二、长时间循环等待：**CAS 长时间不成功的话，会导致一直自旋（死循环），CPU开销较大。
**三、只保证 一个共享变量的原子操作：**
CAS操作单个共享变量赋值时 可以保证原子操作，但多个变量就不行；

#### Atomic 相关类：

AtomicInteger，AtomicBoolean 等 java.util.concurrent包下面的类，只能并发修改一个属性
AtomicInteger 类利用 CAS + volatile 和 native 方法 来保证原子操作，避免 synchronized 的高开销，执行效率大为提升。
内部使用 UnSafe 类 objectFieldOffset() 一个本地方法，可以得到 “原来的值”的内存地址，返回值 valueOffset； value 是一个volatile变量，在内存中可见，JVM 可以保证 任何时刻，任何线程 总能拿到该变量的最新值。

AtomicReference： 
AtomicReference 可以保证 对象操作的原子性；不单单仅限于 Integer, Boolean, Long等类型
和 AtomicInteger 类似，AtomicInteger是 对整数的封装，而 AtomicReference 则是对普通对象的引用，它可以保证在修改对象引用时的线程安全性。 AtomicReference原子性的作用是对  ”对象”进行原子操作；提供一种 读 和 写都是原子性的对象引用变量。

原理： 通过 “volatile” 和 “Unsafe 提供的 CAS函数实现"原子操作
1) value是 volatile类型，保证当某个线程修改 value值时，其他线程看到的 value值都是最新的，即修改之后的volatile的值
2) 通过 CAS设置value，保证当某个线程池通过CAS函数(如compareAndSet)设置value时，操作是原子的，即线程在操作value时不会被中断
需要借用Unsafe类的原子操作
使用场景：当涉及到 比较，设置等，多于一个操作时，使用AtomicReference ；

升级： AtomicStampedReference类通过引入 时间戳作为数据的版本号，来解决ABA 问题。



**悲观锁：**访问共享资源前，先要上锁，认为多线程同时修改共享资源的概率高，容易出现冲突
**互斥锁：**一个线程获得锁，其他线程无法再次获得，挂起等待锁的释放；加锁失败时，会做「线程切换」，当加锁失败的线程再次加锁成功，这一过程会有两次线程上下文切换的成本，性能损耗大。
**自旋锁：**加锁失败时不会主动产生线程切换，忙等待直到获得锁；如果被锁代码执行时间短，那忙等待的时间相对应也短。

**读写锁：**
原理：当「写锁」没有被线程持有时，多个线程能够并发的持有读锁，提高共享资源访问效率；因为「读锁」是用于读取共享资源，多个线程同时持有读锁不会破坏共享资源。一旦「写锁」被线程持有，读线程获取读锁的操作会被阻塞，其他写线程获取写锁的操作也会被阻塞。写锁是独占锁，因为任何时刻只能有一个线程持有写锁，类似互斥锁和自旋锁；读锁是共享锁，因为读锁可以被多个线程同时持有。
应用场景：**在读多写少**

读写锁可以分为「读优先锁」和「写优先锁」
**读优先锁 ：**期望读锁能被更多的线程持有，提高读线程的并发性。方式：当读线程A 持有读锁，写线程B 在获取写锁时会被阻塞，在阻塞过程中，后续来的读线程C 仍可以成功获取读锁，直到读线程A 和C 释放读锁后，写线程B 才可以成功获取读锁。
**写优先锁：**优先服务写线程；方式：读线程A 先持有读锁，写线程B 在获取写锁时会被阻塞，阻塞过程中，后续读线程C 获取读锁时会失败，被阻塞在获取读锁的操作上，这样只要读线程A 释放读锁，写线程B 就可以成功获取读锁。

#### 总结：

读优先锁：对于读线程并发性更好，但可能出现 写线程「饥饿」的现象。
写优先锁：保证写线程不会饿死，但如果一直有写线程获取写锁，读线程也会被「饿死」。
解决：公平读写锁，用队列把获取锁的线程排队，读，写线程都按照先进先出原则加锁，这样就不会出现「饥饿」现象。

#### 乐观锁：

乐观锁：假定冲突的概率很低，先修改共享资源，再验证这段时间 内有没有发生冲突。没有其他线程修改，则操作成功；发现有其他线程修改，则放弃本次操作。
使用场景：**在冲突概率低，加锁成本高**
冲突概率上升，不适合乐观锁，因为解决冲突的重试成本非常高。
注意：加锁代码的范围尽可能的小，加锁的粒度要小，这样执行速度会比较快。



#### ThreadLocal :

数据隔离，填充的数据只属于当前线程，变量数据对别的线程而言是相对隔离的，多线程环境下，防止自己的变量被其它线程修改。

#### 存储结构：

​		value 实际上存储在ThreadLocalMap中，ThreadLocalMap 是ThreadLocal的一个 静态内部类，它的 key是 ThreadLocal实例对象，value是任意Object对象。每个线程实例都对应一个 TheadLocalMap实例，在同一个线程里实例化很多 ThreadLocal来存储很多种类型的值，这些ThreadLocal实例分别作为key，对应各自的value，存储在 Entry table 数组中。
set获取当前线程的ThreadLocalMap，然后往map里添加 KV，K是当前ThreadLocal实例，V是我们传入的value
get获取当前线程对应的私有变量，是之前set或通过 initialValue的值

​		ThreadLocal 能做到线程间的数据隔离，别的线程使用get() 方法没办法拿到其他线程的值. 每个线程 Thread 都维护自己的threadLocals变量，在每个线程创建 ThreadLocal 时，实际上数据是存在 线程Thread的 threadLocals中的 ThreadLocalMap 里，别人没办法拿到，从而实现隔离。

**ThreadLocal内存模型原理：**

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gurotiajlqj60mg0bc3yz02.jpg" alt="37DE7955-FB83-46CD-BC0C-8B432E31AA15" width="550" height="250"/>

线程的一些局部变量和引用：Stack（栈）区；  普通对象：Heap（堆）区
1.线程运行时，定义的 TheadLocal对象被初始化，存储在Heap，同时线程运行的栈区保存指向该实例的引用，也就是图中的ThreadLocalRef。
2.当 ThreadLocal的 set / get 被调用时，虚拟机会根据当前线程引用，查看对应的TheadLocalMap实例是否被创建，如果没有则创建并初始化
3.虚线，表示 key对应的 ThreadLocal实例的引用是个弱引用。

建议：
1.需要 存储 线程私有变量的时
2.需要实现 线程安全的变量时
3.需要 减少线程资源竞争的时

#### ThreadLocalMap底层使用数组，如何解决Hash冲突？

 		一个线程可以有多个 TreadLocal 来存放不同类型的对象，他们都放到当前线程的 ThreadLocalMap里，一个线程一个数组来存。ThreadLocalMap存储时，给每一个ThreadLocal对象一个 threadLocalHashCode，插入过程中根据 ThreadLocal对象的hash值，定位到 table中的位置id ，然后判断当前位置 是否为空，为空就初始化一个Entry对象放在位置id上；
如果位置id 不空，这个Entry对象的 key = 计算出来的hash值，则更新 Entry中的value；
如果位置id 不空，且 Entry 对象的 key !=计算出来的hash值， 则找下一个空位置。
set和get如果冲突严重的话，效率会降低。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gurou020ddj611g0l6ac102.jpg" alt="440644C2-998F-49CA-80DA-BA894E6BD00D" width="650" height="300" />

**ThreadLocal 实例及其值存放在哪？** ThreadLocal实例实际上被其创建的类持有，而ThreadLocal的值是被线程实例持有，位于堆上，只是通过一些技巧将 可见性修改成了线程可见。

如何共享 一个线程的ThreadLocal数据？ 使用 Inheritable ThreadLocal可以实现多个线程访问ThreadLocal的值。例：在主线程中创建一个InheritableThreadLocal实例，在 子线程中得到这个Inheritable ThreadLocal实例设置的值。

#### ThreadLocal 内存泄漏：

ThreadLocalMap 使用的 key 为 ThreadLocal 的弱引用, 而 value 是强引用。如果ThreadLocal 没有被外部强引用，在垃圾回收时 key 会被清理掉，而 value 不会被清理掉。这样 ThreadLocalMap 中会出现 key为null的Entry。假如不做任何措施，value 永远无法被GC 回收，导致内存泄露。
解决：每次使用完 ThreadLocal，调用 remove()方法，清除数据。

ThreadLocalMap的key 为什么设计成弱引用？

如果 key不设置成 弱引用，会造成 entry中 value一样内存泄漏的场景。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1guroufjdoej611a0diwfp02.jpg" alt="A3923F60-AE72-4D55-B680-B9BC54CEDD23" width="550" height="250" />



#### CountDownLatch:

允许一个或 多个线程一直等待，直到 count=0，唤醒所有线程。
一个线程调用await() 时，阻塞当前线程。有线程调用一次 countDown() 时，计数 -1。当 count = 0 时，被阻塞线程全部唤醒。

#### CyclicBarrier  (barrier 障碍, cyclic 循环）

一组线程互相等待，直到所有线程都到达一个同步点。
例：一组运动员比赛 100米，当所有人都准备完成之后，才可以一起开跑。

**CountDownLatch 和CyclicBarrier 区别：**

1.CountDownLatch 是一个线程等待其他线程， CyclicBarrier 多个线程互相等待。
2.CountDownLatch  计数 -1 直到 0，CyclicBarrier 计算 +1，直到指定值。
3.CountDownLatch 是一次性的， CyclicBarrier  可以循环利用。



#### AbstractQueuedSynchronizer（AQS）

ReentrantLock、ReentrantReadWriteLock、CountDownLatch 都是基于AQS实现的

**AQS实现原理：**AQS中维护一个 volatile int state（共享资源）和 一个 FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。
state 初始化 0，多线程条件下线程要执行 临界区的代码，首先获取 state。 某个线程获取成功后， state +1，其他线程再获取的话，由于共享资源已被占用，将阻塞线程放到 FIFO 等待队列 中，等占有 state 的线程执行完，释放资源( state -1)后，会唤醒 FIFO 中下一个等待线程去获取 state。

 state 由于是多线程共享变量，所以定义成 volatile，保证 state 可见性, 并通过 CAS 来保证其并发修改的安全性。FIFO 等待队列是一个双向链表，head 结点代表当前占用的线程，其他节点由于暂时获取不到锁，依次 排队等待锁的释放

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gurouysglyj61100fgwff02.jpg" alt="5A584450-F81D-4DEC-8C2A-8BBEE7DD5371" width="650" height="300" />



AQS定义两种资源共享方式
1.Exclusive（独占）：只有一个线程执行； 如ReentrantLock，可分为公平锁和非公平锁：

**公平锁：按照线程在队列中的排队顺序，先到者先得锁

**非公平锁：获取锁时，无视队列顺序直接抢锁，谁抢到是谁的

2.Share（共享）：多线程同时执行，如CountDownLatch

​		获取独占式锁过程对做个总结: AQS的模板方法acquire通过调用子类自定义实现的tryAcquire获取同步状态失败后->将线程构造成Node节点(addWaiter)->将Node节点添加到同步队列对尾(addWaiter)->节点以自旋的方法获取同步状态(acquirQueued)。在节点自旋获取同步状态时，只有其前驱节点是头节点的时候才会尝试获取同步状态，如果该节点的前驱不是头节点或者该节点的前驱节点是头节点单获取同步状态失败，则判断当前线程需要阻塞，如果需要阻塞则需要被唤醒过后才返回。

AQS 内部维护一个线程的队列。队列由内部的节点组成。
队列的节点为Node,节点分为SHARED和EXCLUSIVE分别时共享模式的节点和独占模式的节点。
节点的等待状态为waitStatus

* CANCELLED(1):取消状态，当线程不再希望获取锁时，设置为取消状态
* SIGNAL(-1):当前节点的后继者处于等待状态，当前节点的线程如果释放或取消了同步状态，通知后继节点
* CONDITION(-2):等待队列的等待状态，当调用signal()时，进入同步队列
* PROPAGATE(-3): 共享模式，同步状态的获取的可传播状态
* 0：初始状态

  AQS共享模式和 独占模式的实现上最大的差别就在于共享模式获取锁后的传播。其他的区别主要还是表现在实现类实现的区别上。通过ReentrantLock和ReentrantReadWriteLock可以了解AQS的独占模式和共享模式，但是要注意将AQS和锁的实现剥离开，弄明白哪些逻辑是AQS实现的，哪些逻辑是锁实现的，同时也思考怎么使用AQS实现其他的特定的线程同步问题。



#### 公平锁和非公平锁：

非公平锁和公平锁的区别：非公平锁性能高于公平锁性能。非公平锁可以减少 CPU唤醒线程的开销，整体的吞吐效率会高点，CPU不必 唤醒所有线程，减少唤起线程的数量
**问题：**非公平锁存在线程饥饿的情况，可能某个线程一直获取不到锁.

#### AQS 组件：

1.Semaphore(信号量)-允许多线程同时访问： synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源
2.CountDownLatch ： 一个同步工具类，协调多线程之间的同步。用来控制线程等待，让某一个线程等待直到倒计时结束，再开始执行。
3.CyclicBarrier ： CyclicBarrier 和 CountDownLatch 类似，可以实现线程间的等待，但功能比 CountDownLatch 更复杂和强大。
主要应用场景和 CountDownLatch 类似，CyclicBarrier 可以让 一组线程到达一个屏障（同步点）时被阻塞，直到 最后一个线程到达屏障时，屏障才会打开，所有被屏障拦截的线程才会继续执行。
CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，参数表示 屏障拦截的线程数量，每个线程调用await()方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。

排他锁：只有一个线程可以访问共享变量； 
共享锁：允许多个线程同时访问
重入锁是排他的；信号量是共享的

**AQS内部结构：**AQS中有一个同步等待队列，保存等待在这个锁上的线程。
为了维护等待在 条件变量上的等待线程，AQS维护一个条件变量等待队列，就是由 Condition.await() 引起阻塞的线程。一个重入锁可以生成 多个条件变量对象，因此一个重入锁 可能有多个条件变量等待队列。每个条件变量对象内部都维护一个等待列表。



<img src="https://tva1.sinaimg.cn/large/008i3skNly1gurowjlkbcj610o0pctbc02.jpg" alt="D087E893-6647-40B7-8FE2-31AC3634D7A5" width="350" height="250" />     <img src="https://tva1.sinaimg.cn/large/008i3skNly1gurovk1mmcj60sk0p4jsr02.jpg" alt="926080CA-1FDD-480D-9FF1-E91DE7EF8034" width="380" height="250" />

同步等待队列，条件变量等待队列，都使用 同一个 Node类作为链表的节点 
Node 中包括链表的上一个元素 prev，下一个元素next 和线程对象thread，对于条件变量等待队列，还使用nextWaiter表示下一个等待在条件变量队列中的节点。

#### Condition ：

Condition是用来替代Object的 wait()，notify() 方法的，使用Condition中await()，signal()可以更加安全，高效的实现线程间协作。

#### Condition 和 wait/notify 比较：

1.Condition 可以精准的 对多个不同条件进行控制，wait/notify 只能和 synchronized关键字一起使用，只能唤醒一个或全部的等待队列；
2.Condition 使用 Lock进行控制，使用后要 unlock()，Condition 有类似于 await 的机制，不会因为加锁而产生死锁问题，底层实现park/ unpark 机制，不会产生死锁，但wait/notify 可能会产生先唤醒再挂起的死锁。

1. 对系统资源占用不同：虽然都释放了CPU，但阻塞的进程仍处于内存中，而挂起的进程通过“对换”技术被换出到外存（磁盘）中。 
2. 发生时机不同：阻塞一般在进程等待资源（IO资源、信号量等）时发生；而挂起是由于用户和系统的需要，例如，终端用户需要暂停程序研究其执行情况或对其进行修改、OS为了提高内存利用率需要将暂时不能运行的进程（处于就绪或阻塞队列的进程）调出到磁盘 
3. 恢复时机不同：阻塞要在等待的资源得到满足（例如获得了锁）后，才会进入就绪状态，等待被调度而执行；被挂起的进程由将其挂起的对象（如用户、系统）在时机符合时（调试结束、被调度进程选中需要重新执行）将其主动激活

一般线程中的阻塞：

A、线程执行了Thread.sleep(int millsecond);方法，当前线程放弃CPU，睡眠一段时间，然后再恢复执行
B、线程执行一段同步代码，但是尚且无法获得相关的同步锁，只能进入阻塞状态，等到获取了同步锁，才能回复执行。
C、线程执行了一个对象的wait()方法，直接进入阻塞状态，等待其他线程执行notify()或者notifyAll()方法。
D、线程执行某些IO操作，因为等待相关的资源而进入了阻塞状态。比如说监听system.in，但是尚且没有收到键盘的输入，则进入阻塞状态。



**非公平锁和公平锁的区别：** 非公平锁性能高于公平锁性能。非公平锁可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量
非公平锁性能虽然优于公平锁，但是会存在导致线程饥饿的情况。在最坏的情况下，可能存在某个线程一直获取不到锁。不过相比性能而言，饥饿问题可以暂时忽略，这可能就是ReentrantLock默认创建非公平锁的原因之一了